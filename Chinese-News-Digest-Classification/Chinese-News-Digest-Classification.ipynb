{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Chinese\\-News\\-Digest\\-Classification\r\n",
    "\r\n",
    "56821 pieces of Chinese news digest fetched from websites\\.\r\n",
    "\r\n",
    "This dataset can be divided into 10 categories: international, cultural, entertainment, sports, finance, automobile, education, technology, real estate, securities\\.\r\n",
    "\r\n",
    "The project implements CNN (Convolutional Neural Networks) to deal with the categories\\."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\r\n",
    "import os\r\n",
    "from multiprocessing import cpu_count\r\n",
    "import numpy as np\r\n",
    "import shutil\r\n",
    "# CPU version used\r\n",
    "# python3 -m pip install paddlepaddle -i https://pypi.tuna.tsinghua.edu.cn/simple\r\n",
    "# PaddlePaddle 1.8.0\r\n",
    "import paddle # Baidu \r\n",
    "import paddle.fluid as fluid\r\n",
    "\r\n",
    "\r\n",
    "# path \r\n",
    "data_root_path = '/home/aistudio/work/'\r\n",
    "# data_root_path = './'\r\n",
    "model_save_path = '/home/aistudio/work/infer/'\r\n",
    "# model_save_path = './infer/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1\\. Data Preparation\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1\\.1 Data Set and Dict \r\n",
    "\r\n",
    "Now we only have one file (the raw data): **news\\_classify\\_data\\.txt**, and we have to prepare the data ready for our CNN model\\. So before building the network, 3 more files have to be created\\. \r\n",
    "\r\n",
    "The function `create_dict` is designed to generate **dict\\_txt\\.txt**, corresponding characters and numbers one by one\\. \r\n",
    "\r\n",
    "After `create_dict`, `create_data_list` will be able to generate a test set: **test_list.txt** and a training set: **train_list.txt**\\."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict created! \n",
      "dataset created! \n"
     ]
    }
   ],
   "source": [
    "# create a dataset \r\n",
    "def create_data_list(data_root_path):\r\n",
    "    # initialize and clear the existing content \r\n",
    "    with open(data_root_path + 'test_list.txt', 'w') as f:\r\n",
    "        pass \r\n",
    "    with open(data_root_path + \"train_list.txt\", 'w') as f:\r\n",
    "        pass \r\n",
    "\r\n",
    "    with open(os.path.join(data_root_path, 'dict_txt.txt'), 'r', encoding='UTF-8') as f_data:\r\n",
    "        dict_txt = eval(f_data.readlines()[0])\r\n",
    "    with open(os.path.join(data_root_path, 'news_classify_data.txt'), 'r', encoding='UTF-8') as f_data:\r\n",
    "        lines = f_data.readlines()\r\n",
    "\r\n",
    "    i = 0 \r\n",
    "    for line in lines:\r\n",
    "        title = line.split(\"_!_\")[-1].replace('\\n', '') # the last one \r\n",
    "        l = line.split(\"_!_\")[1] # the second one \r\n",
    "        labs = \"\"\r\n",
    "        # a way to select test set and training set\r\n",
    "        if i%10 == 0:\r\n",
    "            with open(os.path.join(data_root_path, \"test_list.txt\"), 'a', encoding='UTF-8') as f_test:\r\n",
    "                for s in title:\r\n",
    "                    lab = str(dict_txt[s])   \r\n",
    "                    labs = labs + lab + ','\r\n",
    "                labs = labs[:-1] \r\n",
    "                labs = labs + '\\t' + l + '\\n'\r\n",
    "                f_test.write(labs)\r\n",
    "        else:\r\n",
    "            with open(os.path.join(data_root_path, 'train_list.txt'), 'a', encoding='UTF-8') as f_train:\r\n",
    "                for s in title:\r\n",
    "                    lab = str(dict_txt[s])\r\n",
    "                    labs = labs + lab + ','\r\n",
    "                labs = labs[:-1]\r\n",
    "                labs = labs + '\\t' + l + '\\n'\r\n",
    "                f_train.write(labs) \r\n",
    "        i += 1\r\n",
    "    print('dataset created! ')\r\n",
    "    \r\n",
    "\r\n",
    "# create a dict \r\n",
    "def create_dict(data_path, dict_path):\r\n",
    "    dict_set = set()\r\n",
    "    # read the data \r\n",
    "    with open(data_path, 'r', encoding='UTF-8') as f:\r\n",
    "        lines = f.readlines()\r\n",
    "    # turn the data into tuple \r\n",
    "    for line in lines:\r\n",
    "        title = line.split(\"_!_\")[-1].replace('\\n', '')\r\n",
    "        for s in title:\r\n",
    "            dict_set.add(s)\r\n",
    "    # turn tuple into dict\r\n",
    "    # one Chinese character corresponds to one number\r\n",
    "    dict_list = list()\r\n",
    "    i = 0 \r\n",
    "    for s in dict_set:\r\n",
    "        dict_list.append([s, i])\r\n",
    "        i = i + 1\r\n",
    "    # add unknown character \r\n",
    "    # replace all unknown characters with `<unk>`\r\n",
    "    dict_txt = dict(dict_list)\r\n",
    "    end_dict = {'<unk>': i}\r\n",
    "    dict_txt.update(end_dict)\r\n",
    "    # save \r\n",
    "    with open(dict_path, 'w', encoding='UTF-8') as f:\r\n",
    "        f.write(str(dict_txt))\r\n",
    "    print('dict created! ')\r\n",
    "\r\n",
    "\r\n",
    "def get_dict_length(dict_path):\r\n",
    "    with open(dict_path, 'r', encoding='UTF-8') as f:\r\n",
    "        line = eval(f.readlines()[0])\r\n",
    "    return len(line.keys())\r\n",
    "\r\n",
    "\r\n",
    "if __name__ == '__main__':\r\n",
    "    data_path = os.path.join(data_root_path, 'news_classify_data.txt')\r\n",
    "    dict_path = os.path.join(data_root_path, 'dict_txt.txt')\r\n",
    "\r\n",
    "    # create dict \r\n",
    "    create_dict(data_path=data_path, dict_path=dict_path)\r\n",
    "    # create dataset \r\n",
    "    create_data_list(data_root_path=data_root_path)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1\\.2 Data Reader \r\n",
    "\r\n",
    "`paddle.reader.xmap_readers()`: use user\\-defined **mapper** to map the samples returned by the **reader** to the output queue through multi\\-threading\\.\r\n",
    "\r\n",
    "Now we have to create **2 readers**: `train_reader` and `test_reader`\\."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pretreatment\r\n",
    "def data_mapper(sample):\r\n",
    "    data, label = sample \r\n",
    "    data = [int(data) for data in data.split(\",\")]\r\n",
    "    return data, int(label)\r\n",
    "\r\n",
    "\r\n",
    "# create `test_reader`\r\n",
    "def test_reader(test_list_path):\r\n",
    "    def reader():\r\n",
    "        with open(test_list_path, 'r') as f:\r\n",
    "            lines = f.readlines()\r\n",
    "            for line in lines:\r\n",
    "                data, label = line.split('\\t')\r\n",
    "                yield data, label \r\n",
    "\r\n",
    "    return paddle.reader.xmap_readers(data_mapper, reader, cpu_count(), 1024)\r\n",
    "\r\n",
    "\r\n",
    "# create `train_reader`\r\n",
    "def train_reader(train_list_path):\r\n",
    "    def reader():\r\n",
    "        with open(train_list_path, 'r') as f:\r\n",
    "            lines = f.readlines()\r\n",
    "            # scramble data\r\n",
    "            np.random.shuffle(lines)\r\n",
    "            for line in lines:\r\n",
    "                data, label = line.split('\\t')\r\n",
    "                yield data, label \r\n",
    "\r\n",
    "    return paddle.reader.xmap_readers(data_mapper, reader, cpu_count(), 1024)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2\\. CNN\r\n",
    "\r\n",
    "Input word vector sequence to generate a **feature map**\\. Then, use the **max pooling over time** for the feature map to obtain the feature of entire sentence corresponding to this convolution kernel\\. Finally, combine the features obtained by all the convolution kernels to obtain a fixed\\-length vector representation of the text\\.\r\n",
    "\r\n",
    "In actual application scenarios, we use multiple convolution kernels to process sentences and convolution kernels with the same window size are stacked to form a matrix\\. In this way, calculations can be completed more efficiently. (Of course we can use convolution kernels of different window sizes to process sentences\\.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2\\.1 Create and Configure the network \r\n",
    "  \r\n",
    "- Define the network\r\n",
    "- Define cost function\r\n",
    "- Define optimizer\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create CNN\r\n",
    "def CNN_net(data, dict_dim, class_dim=10, emb_dim=128, hid_dim=128, hid_dim2=98):  \r\n",
    "    emb = fluid.layers.embedding(input=data, size=[dict_dim, emb_dim])\r\n",
    "    conv_3 = fluid.nets.sequence_conv_pool(\r\n",
    "        input = emb,\r\n",
    "        num_filters = hid_dim,\r\n",
    "        filter_size = 3,\r\n",
    "        act = \"tanh\", \r\n",
    "        pool_type = 'sqrt', \r\n",
    "    )\r\n",
    "    conv_4 = fluid.nets.sequence_conv_pool(\r\n",
    "        input = emb, \r\n",
    "        num_filters = hid_dim2, \r\n",
    "        filter_size = 4, \r\n",
    "        act = 'tanh', \r\n",
    "        pool_type = 'sqrt', \r\n",
    "    )\r\n",
    "    output = fluid.layers.fc(\r\n",
    "        input = [conv_3, conv_4], \r\n",
    "        size = class_dim, \r\n",
    "        act = 'softmax', \r\n",
    "    )\r\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define input\r\n",
    "# `lod_level` != 0: specify the input data as a sequence \r\n",
    "words = fluid.layers.data(name='words', shape=[1], dtype='int64', lod_level=1)\r\n",
    "label = fluid.layers.data(name='label', shape=[1], dtype='int64')\r\n",
    "# get the length of the data dict\r\n",
    "dict_dim = get_dict_length(os.path.join(data_root_path, 'dict_txt.txt'))\r\n",
    "# get CNN model \r\n",
    "# model = CNN_net(words, dict_dim, 15)\r\n",
    "# get classifier \r\n",
    "model = CNN_net(words, dict_dim)\r\n",
    "# cost function and accuracy \r\n",
    "cost = fluid.layers.cross_entropy(input=model, label=label)\r\n",
    "avg_cost = fluid.layers.mean(cost)\r\n",
    "acc = fluid.layers.accuracy(input=model, label=label)\r\n",
    "\r\n",
    "# prediction \r\n",
    "test_program = fluid.default_main_program().clone(for_test=True)\r\n",
    "\r\n",
    "# define optimizer \r\n",
    "optimizer = fluid.optimizer.AdagradOptimizer(learning_rate=0.002)\r\n",
    "opt = optimizer.minimize(avg_cost)\r\n",
    "\r\n",
    "# create an executor \r\n",
    "# as is mentioned at the beginning, we use CPU version (slow)\r\n",
    "# place = fluid.CUDAPlace()\r\n",
    "place = fluid.CPUPlace()\r\n",
    "\r\n",
    "exe = fluid.Executor(place)\r\n",
    "# initialize params \r\n",
    "exe.run(fluid.default_startup_program())\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2\\.2 Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass: 0, Batch: 0, Cost: 2.30808, Acc: 0.09375\n",
      "Pass: 0, Batch: 100, Cost: 1.16889, Acc: 0.61719\n",
      "Pass: 0, Batch: 200, Cost: 0.95838, Acc: 0.69531\n",
      "Pass: 0, Batch: 300, Cost: 0.78453, Acc: 0.75781\n",
      "Test: 0, Cost: 0.81800, Acc: 0.73809\n",
      "Pass: 1, Batch: 0, Cost: 0.75859, Acc: 0.80469\n",
      "Pass: 1, Batch: 100, Cost: 0.86124, Acc: 0.67969\n",
      "Pass: 1, Batch: 200, Cost: 0.69970, Acc: 0.80469\n",
      "Pass: 1, Batch: 300, Cost: 0.63052, Acc: 0.80469\n",
      "Test: 1, Cost: 0.75982, Acc: 0.75293\n",
      "Pass: 2, Batch: 0, Cost: 0.63957, Acc: 0.79688\n",
      "Pass: 2, Batch: 100, Cost: 0.69336, Acc: 0.79688\n",
      "Pass: 2, Batch: 200, Cost: 0.57135, Acc: 0.80469\n",
      "Pass: 2, Batch: 300, Cost: 0.67595, Acc: 0.75781\n",
      "Test: 2, Cost: 0.73990, Acc: 0.75579\n",
      "Pass: 3, Batch: 0, Cost: 0.67695, Acc: 0.79688\n",
      "Pass: 3, Batch: 100, Cost: 0.51714, Acc: 0.83594\n",
      "Pass: 3, Batch: 200, Cost: 0.59802, Acc: 0.82031\n",
      "Pass: 3, Batch: 300, Cost: 0.49044, Acc: 0.81250\n",
      "Test: 3, Cost: 0.72521, Acc: 0.76239\n",
      "Pass: 4, Batch: 0, Cost: 0.49337, Acc: 0.85156\n",
      "Pass: 4, Batch: 100, Cost: 0.57643, Acc: 0.81250\n",
      "Pass: 4, Batch: 200, Cost: 0.59582, Acc: 0.78125\n",
      "Pass: 4, Batch: 300, Cost: 0.74973, Acc: 0.75781\n",
      "Test: 4, Cost: 0.72200, Acc: 0.75892\n",
      "Pass: 5, Batch: 0, Cost: 0.61845, Acc: 0.78906\n",
      "Pass: 5, Batch: 100, Cost: 0.49068, Acc: 0.84375\n",
      "Pass: 5, Batch: 200, Cost: 0.48436, Acc: 0.82031\n",
      "Pass: 5, Batch: 300, Cost: 0.72536, Acc: 0.75000\n",
      "Test: 5, Cost: 0.71634, Acc: 0.76239\n",
      "Pass: 6, Batch: 0, Cost: 0.72363, Acc: 0.75000\n",
      "Pass: 6, Batch: 100, Cost: 0.59225, Acc: 0.80469\n",
      "Pass: 6, Batch: 200, Cost: 0.61890, Acc: 0.78125\n",
      "Pass: 6, Batch: 300, Cost: 0.61041, Acc: 0.77344\n",
      "Test: 6, Cost: 0.71147, Acc: 0.76413\n",
      "Pass: 7, Batch: 0, Cost: 0.71022, Acc: 0.75781\n",
      "Pass: 7, Batch: 100, Cost: 0.51805, Acc: 0.82812\n",
      "Pass: 7, Batch: 200, Cost: 0.61236, Acc: 0.82031\n",
      "Pass: 7, Batch: 300, Cost: 0.68069, Acc: 0.78125\n",
      "Test: 7, Cost: 0.70784, Acc: 0.76674\n",
      "Pass: 8, Batch: 0, Cost: 0.58608, Acc: 0.83594\n",
      "Pass: 8, Batch: 100, Cost: 0.60431, Acc: 0.79688\n",
      "Pass: 8, Batch: 200, Cost: 0.55773, Acc: 0.80469\n",
      "Pass: 8, Batch: 300, Cost: 0.61073, Acc: 0.78125\n",
      "Test: 8, Cost: 0.71368, Acc: 0.76638\n",
      "Pass: 9, Batch: 0, Cost: 0.51035, Acc: 0.84375\n",
      "Pass: 9, Batch: 100, Cost: 0.47972, Acc: 0.82812\n",
      "Pass: 9, Batch: 200, Cost: 0.42585, Acc: 0.87500\n",
      "Pass: 9, Batch: 300, Cost: 0.60230, Acc: 0.76562\n",
      "Test: 9, Cost: 0.70767, Acc: 0.76743\n",
      "Model saved! \n"
     ]
    }
   ],
   "source": [
    "# readers for training set and test set \r\n",
    "train_r = paddle.batch(reader=train_reader(os.path.join(data_root_path, 'train_list.txt')), batch_size=128)\r\n",
    "test_r = paddle.batch(reader=test_reader(os.path.join(data_root_path, 'test_list.txt')), batch_size=128)\r\n",
    " \r\n",
    "# define data mapper \r\n",
    "feeder = fluid.DataFeeder(place=place, feed_list=[words, label])\r\n",
    "\r\n",
    "EPOCH_NUM = 10\r\n",
    "\r\n",
    "\r\n",
    "# training  \r\n",
    "for pass_id in range(EPOCH_NUM):\r\n",
    "    # train \r\n",
    "    for batch_id, data in enumerate(train_r()):\r\n",
    "        train_cost, train_acc = exe.run(\r\n",
    "            program = fluid.default_main_program(), \r\n",
    "            feed = feeder.feed(data), \r\n",
    "            fetch_list = [avg_cost, acc], \r\n",
    "            )\r\n",
    "        if batch_id%100 == 0:\r\n",
    "            print(\"Pass: %d, Batch: %d, Cost: %0.5f, Acc: %0.5f\" % (pass_id, batch_id, train_cost[0], train_acc[0]))\r\n",
    "    # test \r\n",
    "    test_costs = list()\r\n",
    "    test_accs = list()\r\n",
    "    for batch_id, data in enumerate(test_r()):\r\n",
    "        test_cost, test_acc = exe.run(\r\n",
    "            program = test_program,\r\n",
    "            feed = feeder.feed(data), \r\n",
    "            fetch_list = [avg_cost, acc],  \r\n",
    "        )\r\n",
    "        test_costs.append(test_cost[0])\r\n",
    "        test_accs.append(test_acc[0]) \r\n",
    "    # average test cost and accuracy \r\n",
    "    test_cost = (sum(test_costs)) / len(test_costs)\r\n",
    "    test_acc = (sum(test_accs)) / len(test_accs)\r\n",
    "    print('Test: %d, Cost: %0.5f, Acc: %0.5f' % (pass_id, test_cost, test_acc))\r\n",
    "\r\n",
    "\r\n",
    "# save the prediction model \r\n",
    "if not os.path.exists(model_save_path):\r\n",
    "    os.makedirs(model_save_path)\r\n",
    "fluid.io.save_inference_model(model_save_path, feeded_var_names=[words.name], target_vars=[model], executor=exe)\r\n",
    "print('Model saved! ')\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 0, Name: 文化, Confidence: 0.912935\n",
      "Prediction: 8, Name: 国际, Confidence: 0.432612\n"
     ]
    }
   ],
   "source": [
    "# infer with the model \r\n",
    "# create executor \r\n",
    "place = fluid.CPUPlace()\r\n",
    "exe = fluid.Executor(place)\r\n",
    "exe.run(fluid.default_startup_program())\r\n",
    "\r\n",
    "# import prediction program, data lists, classifier from model \r\n",
    "[infer_program, feeded_var_names, target_var] = fluid.io.load_inference_model(dirname=model_save_path, executor=exe)\r\n",
    "\r\n",
    "\r\n",
    "# load data \r\n",
    "def get_data(sentence):\r\n",
    "    # read data dict \r\n",
    "    with open(os.path.join(data_root_path, 'dict_txt.txt'), 'r', encoding='UTF-8') as f_data:\r\n",
    "        dict_txt = eval(f_data.readlines()[0])\r\n",
    "    f_data.close()\r\n",
    "    dict_txt = dict(dict_txt)\r\n",
    "    # turn string into list \r\n",
    "    keys = dict_txt.keys()\r\n",
    "    data = list()\r\n",
    "    for s in sentence:\r\n",
    "        if not s in keys:\r\n",
    "            s = '<unk>'\r\n",
    "        data.append(int(dict_txt[s]))\r\n",
    "    return data \r\n",
    "\r\n",
    "\r\n",
    "data = list()\r\n",
    "# get graph data \r\n",
    "data1 = get_data(\"在获得诺贝尔文学奖7年之后，莫言15日晚间在山西汾阳贾家庄如是说\")\r\n",
    "data2 = get_data('综合“今日美国”、《世界日报》等当地媒体报道，芝加哥河滨警察局表示，')\r\n",
    "data.append(data1)\r\n",
    "data.append(data2)\r\n",
    "\r\n",
    "# how many words in each sentence \r\n",
    "base_shape = [[len(c) for c in data]]\r\n",
    "\r\n",
    "# generate prediction data \r\n",
    "tensor_words = fluid.create_lod_tensor(data, base_shape, place)\r\n",
    "\r\n",
    "# generate prediction \r\n",
    "result = exe.run(\r\n",
    "    program = infer_program,\r\n",
    "    feed = {feeded_var_names[0]: tensor_words},\r\n",
    "    fetch_list = target_var, \r\n",
    "    )\r\n",
    "\r\n",
    "# categories \r\n",
    "names = ['文化', '娱乐', '体育', '财经', '房产', '汽车', '教育', '科技', '国际', '证券']\r\n",
    "\r\n",
    "# show label with the greatest confidence \r\n",
    "for i in range(len(data)):\r\n",
    "    lab = np.argsort(result)[0][i][-1]\r\n",
    "    print(\"Prediction: %d, Name: %s, Confidence: %f\" % (lab, names[lab], result[0][i][lab]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 1.8.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
